# 第十一章 Agentic-RL

## 章节概述

本章将介绍如何训练智能体，从基础的 SFT 到高级的 RLHF 和 GRPO，帮助你掌握智能体的训练方法。

## 学习目标

- 理解智能体训练的基本原理
- 掌握 SFT、RLHF、GRPO 的区别
- 了解训练流程和工具
- 能够进行简单的模型微调

---

## 智能体训练概述

### 为什么需要训练？

```
┌─────────────────────────────────────────────────────────────┐
│              预训练模型 vs Agent模型                          │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  预训练 LLM:                                                │
│  · 通用语言能力                                             │
│  · 知识广泛但不够深入                                        │
│  · 缺乏特定任务能力                                          │
│  · 不会使用工具                                              │
│                                                             │
│  Agent 模型:                                               │
│  · 专门的推理能力                                            │
│  · 工具使用技能                                              │
│  · 特定任务优化                                              │
│  · 更好的遵循指令                                            │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 训练方法对比

```
训练方法对比:

┌──────────────────────────────────────────────────────────┐
│  SFT (监督微调)                                            │
│  ┌────────────────────────────────────────────────────┐   │
│  │  数据: (输入, 目标输出) 对                           │   │
│  │  方法: 监督学习，预测下一个token                       │   │
│  │  优点: 简单、可控                                       │   │
│  │  缺点: 需要大量标注数据                                 │   │
│  └────────────────────────────────────────────────────┘   │
└──────────────────────────────────────────────────────────┘

┌──────────────────────────────────────────────────────────┐
│  RLHF (人类反馈强化学习)                                     │
│  ┌────────────────────────────────────────────────────┐   │
│  │  数据: 人类对输出的偏好排序                           │   │
│  │  方法: 强化学习，优化奖励模型                         │   │
│  │  优点: 符合人类偏好                                   │   │
│  │  缺点: 复杂、成本高                                   │   │
│  └────────────────────────────────────────────────────┘   │
└──────────────────────────────────────────────────────────┘

┌──────────────────────────────────────────────────────────┐
│  GRPO (组相对策略优化)                                     │
│  ┌────────────────────────────────────────────────────┐   │
│  │  数据: 多个输出成组                                 │   │
│  │  方法: 组内相对比较，无需价值模型                      │   │
│  │  优点: 更高效、更稳定                                 │   │
│  │  缺点: 较新方法，生态不成熟                           │   │
│  └────────────────────────────────────────────────────┘   │
└──────────────────────────────────────────────────────────┘
```

---

## SFT (监督微调)

### 原理

```
SFT 训练流程:

预训练模型
    │
    ▼
┌─────────────────────────────────────────────────────┐
│  准备训练数据                                          │
│  ┌─────────────────────────────────────────────────┐ │
│  │ 输入: "帮我写一个快速排序"                       │ │
│  │ 目标: "def quicksort(arr):..."                   │ │
│  └─────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────┘
    │
    ▼
┌─────────────────────────────────────────────────────┐
│  监督训练                                              │
│  ┌─────────────────────────────────────────────────┐ │
│  │ 损失函数: CrossEntropy(prediction, target)     │ │
│  │                                                 │ │
│  │ 输入 ──→ [模型] ──→ 预测 ──→ 对比目标 ──→ 计算损失 │ │
│  │                                                 │ │
│  │ 反向传播更新参数                                   │ │
│  └─────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────┘
    │
    ▼
微调后的模型
```

### 数据准备

```
数据格式设计:

训练样本:
{
  "messages": [
    {
      "role": "system",
      "content": "你是一个编程助手"
    },
    {
      "role": "user",
      "content": "写一个快速排序"
    },
    {
      "role": "assistant",
      "content": "def quicksort(arr):..."
    }
  ]
}

数据来源:
1. 公开数据集 (如 ShareGPT)
2. 自己构造 (人工标注)
3. 合成数据 (使用强模型生成)
```

---

## RLHF (人类反馈强化学习)

### 三阶段流程

```
RLHF 训练流程:

┌─────────────────────────────────────────────────────────────┐
│  阶段1: SFT (有监督微调)                                      │
│                                                             │
│  预训练模型 ──→ SFT数据 ──→ 微调模型                          │
│                                                             │
│  目标: 让模型学会生成对话                                     │
└─────────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────┐
│  阶段2: RM (奖励模型训练)                                     │
│                                                             │
│  SFT模型 ──→ 生成多个回答 ──→ 人类排序 ──→ 训练奖励模型       │
│                                                             │
│  示例:                                                      │
│  问题: "如何学习编程？"                                     │
│  回答A (排序3): "多看视频..."                               │
│  回答B (排序1): "建议从基础开始..."                          │
│  回答C (排序2): "选择一门语言..."                           │
│                                                             │
└─────────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────┐
│  阶段3: PPO (强化学习)                                       │
│                                                             │
│  SFT模型 ──→ 生成回答 ──→ RM打分 ──→ PPO优化 ──→ 最终模型     │
│                                                             │
│  目标: 生成人类更偏好的回答                                   │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 奖励模型

```
奖励模型训练:

输入: (问题, 回答A, 回答B)
    │
    ▼
┌─────────────────────────────────────────────────────┐
│  编码层                                               │
│  ┌─────────────────────────────────────────────────┐ │
│  │ 问题 + 回答A ──→ [编码器] ──→ 向量A            │ │
│  │ 问题 + 回答B ──→ [编码器] ──→ 向量B            │ │
│  └─────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────┘
    │
    ▼
┌─────────────────────────────────────────────────────┐
│  评分层                                               │
│  ┌─────────────────────────────────────────────────┐ │
│  │ Score = sigmoid(向量A - 向量B)                  │ │
│  │                                                 │ │
│  │ Score > 0.5: A比B好                             │ │
│  │ Score < 0.5: B比A好                             │ │
│  └─────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────┘
```

---

## GRPO (组相对策略优化)

### 核心思想

```
GRPO vs 传统方法:

传统 PPO:
需要价值函数 (Value Function)
计算复杂

GRPO:
不需要价值函数
成组比较，更简单高效
```

### 算法流程

```
GRPO 训练流程:

对于每个问题:
    │
    ▼
┌─────────────────────────────────────────────────────┐
│  生成一组回答 (Group)                                │
│  ┌─────────────────────────────────────────────────┐ │
│  │ 回答1, 回答2, 回答3, ..., 回答N                 │ │
│  │ (N通常为4-8)                                    │ │
│  └─────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────┘
    │
    ▼
┌─────────────────────────────────────────────────────┐
│  评估每个回答                                         │
│  ┌─────────────────────────────────────────────────┐ │
│  │ Reward1, Reward2, ..., RewardN                  │ │
│  │                                                 │ │
│  │ 优势函数 (Advantage):                          │ │
│  │ A_i = Reward_i - mean(Rewards)                 │ │
│  └─────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────┘
    │
    ▼
┌─────────────────────────────────────────────────────┐
│  优化策略                                             │
│  ┌─────────────────────────────────────────────────┐ │
│  │ 增加高奖励回答的概率                               │ │
│  │ 降低低奖励回答的概率                               │ │
│  └─────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────┘
```

---

## 训练工具

### 主流框架

```
训练工具对比:

┌──────────────────────────────────────────────────────────┐
│  LoRA/QLoRA                                              │
│  · 高效微调                                              │
│  · 只训练少量参数                                        │
│  · 显存需求低                                            │
│  · 适合资源有限场景                                       │
└──────────────────────────────────────────────────────────┘

┌──────────────────────────────────────────────────────────┐
│  DeepSpeed                                                │
│  · 分布式训练                                            │
│  · 大规模并行                                             │
│  · 显存优化                                              │
│  · 适合大模型训练                                         │
└──────────────────────────────────────────────────────────┘

┌──────────────────────────────────────────────────────────┐
│  Unsloth                                                  │
│  · 更快的训练                                             │
│  · 显存占用更少                                          │
│  · 易于使用                                              │
│  · 适合快速实验                                           │
└──────────────────────────────────────────────────────────┘
```

---

## 实践建议

### 数据质量

```
数据质量要点:

┌────────────────────────────────────┐
│  1. 多样性                         │
│     不同类型、难度、领域             │
├────────────────────────────────────┤
│  2. 准确性                         │
│     答案必须正确                     │
├────────────────────────────────────┤
│  3. 一致性                         │
│     标注标准统一                     │
├────────────────────────────────────┤
│  4. 去重                           │
│     避免重复数据                     │
└────────────────────────────────────┘
```

### 超参数调优

```
关键超参数:

学习率: 1e-5 ~ 1e-4
Batch Size: 根据显存调整
Epochs: 1-3 (避免过拟合)
Warmup: 10% 训练步数
LoRA Rank: 8-64
```

---

## 练习作业

### 基础练习
1. 准备 SFT 训练数据
2. 了解 LoRA 原理
3. 使用工具进行简单微调

### 进阶练习
4. 设计 RLHF 数据收集流程
5. 实现 GRPO 训练脚本
6. 对比不同训练方法效果

### 挑战练习
7. 从零训练特定任务 Agent
8. 优化训练效率
9. 构建自动化训练流程

## 学习资源

### 相关资料
- "Training language models to follow instructions with human feedback"
- LoRA 论文
- GRPO 论文

### 工具文档
- HuggingFace Transformers
- PEFT (Parameter-Efficient Fine-Tuning)
- Axolotl

## 下一步

完成本章学习后，进入：
- [第12章：性能评估](../ch12-evaluation/) - 学习评估方法

## 学习检查

- [ ] 理解 SFT、RLHF、GRPO 的区别
- [ ] 掌握智能体训练的基本流程
- [ ] 了解主流训练工具
- [ ] 能够准备训练数据
- [ ] 完成章节练习题
