# Reading 1: Observability's Three Pillars and AI Applications
# å¯è§‚æµ‹æ€§ä¸‰å¤§æ”¯æŸ±ä¸ AI åº”ç”¨

> **Week 9 Reading #1**
> **ä¸»é¢˜**: æ·±å…¥ç†è§£å¯è§‚æµ‹æ€§çš„ä¸‰å¤§æ”¯æŸ±åŠå…¶åœ¨ AI åº”ç”¨ä¸­çš„å®è·µ
> **é¢„è®¡é˜…è¯»æ—¶é—´**: 75-90 åˆ†é’Ÿ

---

## ğŸ“š å¯¼è¯»

åœ¨ç°ä»£è½¯ä»¶ç³»ç»Ÿä¸­ï¼Œ"å¯è§‚æµ‹æ€§" (Observability) å·²ç»è¶…è¶Šäº†ä¼ ç»Ÿçš„"ç›‘æ§" (Monitoring) æ¦‚å¿µã€‚ç‰¹åˆ«æ˜¯åœ¨ AI é©±åŠ¨çš„åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬éœ€è¦æ›´æ·±å…¥åœ°ç†è§£ç³»ç»Ÿè¡Œä¸ºã€‚æœ¬æ–‡å…¨é¢ä»‹ç»å¯è§‚æµ‹æ€§çš„ä¸‰å¤§æ”¯æŸ±â€”â€”Metricsã€Logsã€Tracesâ€”â€”ä»¥åŠå¦‚ä½•åˆ©ç”¨ AI æŠ€æœ¯æå‡å¯è§‚æµ‹æ€§èƒ½åŠ›ï¼Œå¸®åŠ©ä½ ï¼š

1. **ç†è§£æ ¸å¿ƒæ¦‚å¿µ** - å¯è§‚æµ‹æ€§çš„ä¸‰å¤§æ”¯æŸ±åŠå…¶åŒºåˆ«
2. **æŒæ¡å·¥å…·æ ˆ** - Prometheusã€Grafanaã€OpenTelemetry ç­‰å·¥å…·
3. **åº”ç”¨ AI æŠ€æœ¯** - ä½¿ç”¨ AI è¿›è¡Œå¼‚å¸¸æ£€æµ‹å’Œæ ¹å› åˆ†æ
4. **å®è·µè½åœ°** - åœ¨ AI åº”ç”¨ä¸­å®æ–½å®Œæ•´çš„å¯è§‚æµ‹æ€§æ–¹æ¡ˆ

---

## ğŸ¯ å­¦ä¹ ç›®æ ‡

é˜…è¯»å®Œæœ¬æ–‡åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

- âœ… ç†è§£å¯è§‚æµ‹æ€§çš„ä¸‰å¤§æ”¯æŸ±åŠå…¶å„è‡ªçš„ä»·å€¼
- âœ… æŒæ¡ RED å’Œ USE æ–¹æ³•è®º
- âœ… å®æ–½ç»“æ„åŒ–æ—¥å¿—å’Œåˆ†å¸ƒå¼è¿½è¸ª
- âœ… ä½¿ç”¨ AI è¿›è¡Œå¼‚å¸¸æ£€æµ‹å’Œæ ¹å› åˆ†æ
- âœ… æ„å»ºå®Œæ•´çš„å¯è§‚æµ‹æ€§å¹³å°

---

## ç¬¬ä¸€éƒ¨åˆ†ï¼šå¯è§‚æµ‹æ€§åŸºç¡€

### 1. ä»€ä¹ˆæ˜¯å¯è§‚æµ‹æ€§ï¼Ÿ

#### å®šä¹‰

**å¯è§‚æµ‹æ€§ (Observability)**: é€šè¿‡ç³»ç»Ÿçš„å¤–éƒ¨è¾“å‡ºï¼ˆæŒ‡æ ‡ã€æ—¥å¿—ã€è¿½è¸ªï¼‰æ¥æ¨æ–­ç³»ç»Ÿå†…éƒ¨çŠ¶æ€çš„èƒ½åŠ›ã€‚

#### ä¸ç›‘æ§çš„åŒºåˆ«

```python
# ç›‘æ§ (Monitoring)
# ä¸»åŠ¨æ£€æŸ¥é¢„å®šä¹‰çš„æŒ‡æ ‡
class Monitoring:
    def __init__(self):
        self.metrics = ["cpu_usage", "memory_usage", "response_time"]

    def check(self):
        """æ£€æŸ¥é¢„å®šä¹‰çš„æŒ‡æ ‡æ˜¯å¦è¶…è¿‡é˜ˆå€¼"""
        for metric in self.metrics:
            value = self.get_metric(metric)
            if value > self.get_threshold(metric):
                self.alert(f"{metric} exceeds threshold")

# å¯è§‚æµ‹æ€§ (Observability)
# é€šè¿‡æ•°æ®ç†è§£ç³»ç»Ÿå†…éƒ¨çŠ¶æ€
class Observability:
    def __init__(self):
        self.metrics = Metrics()    # æŒ‡æ ‡
        self.logs = Logs()           # æ—¥å¿—
        self.traces = Traces()       # è¿½è¸ª

    def understand(self):
        """ç†è§£ç³»ç»Ÿä¸ºä»€ä¹ˆä¼šè¿™æ ·"""
        # ä¸‰å¤§æ”¯æŸ±ç»“åˆï¼Œæ¨æ–­ç³»ç»ŸçŠ¶æ€
        context = self.correlate(
            metrics=self.metrics.get_all(),
            logs=self.logs.search(),
            traces=self.traces.get_flow()
        )
        return self.analyze(context)
```

#### æ ¸å¿ƒä»·å€¼

```
å¯è§‚æµ‹æ€§çš„ä»·å€¼:

1. å¿«é€Ÿå®šä½é—®é¢˜
   - ä»"ç³»ç»Ÿå‡ºé”™äº†"
   - åˆ°"å“ªä¸ªæœåŠ¡ã€å“ªä¸ªæ¥å£ã€å“ªä¸ªé€»è¾‘å‡ºé”™äº†"

2. ç†è§£ç³»ç»Ÿè¡Œä¸º
   - ä¸ä»…æ˜¯"å‡ºé—®é¢˜äº†"
   - è€Œæ˜¯"ä¸ºä»€ä¹ˆä¼šå‡ºé—®é¢˜"

3. æ•°æ®é©±åŠ¨å†³ç­–
   - ä¸æ˜¯é çŒœæµ‹
   - è€Œæ˜¯é æ•°æ®

4. é¢„é˜²æ€§ç»´æŠ¤
   - ä¸æ˜¯è¢«åŠ¨å“åº”
   - è€Œæ˜¯ä¸»åŠ¨é¢„é˜²
```

---

## ç¬¬äºŒéƒ¨åˆ†ï¼šä¸‰å¤§æ”¯æŸ±è¯¦è§£

### 1. Metrics (æŒ‡æ ‡)

#### å®šä¹‰

**Metrics**: æ•°å€¼å‹çš„æ—¶é—´åºåˆ—æ•°æ®ï¼Œè¡¨ç¤ºç³»ç»Ÿåœ¨æŸä¸ªæ—¶é—´ç‚¹çš„çŠ¶æ€ã€‚

#### å…³é”®ç‰¹å¾

- **æ•°å€¼å‹**: å¯ä»¥ç”¨æ•°å­—è¡¨ç¤º
- **æ—¶é—´åºåˆ—**: éšæ—¶é—´å˜åŒ–
- **èšåˆæ€§**: å¯ä»¥èšåˆï¼ˆæ±‚å’Œã€å¹³å‡ç­‰ï¼‰
- **å®æ—¶æ€§**: åæ˜ å½“å‰çŠ¶æ€

#### æ ¸å¿ƒæ–¹æ³•

##### RED æ–¹æ³•

```python
# RED æ–¹æ³• - é€‚ç”¨äºè¯·æ±‚é©±åŠ¨çš„ç³»ç»Ÿï¼ˆå¦‚ Web æœåŠ¡ï¼‰
class REDMetrics:
    """
    Rate: è¯·æ±‚ç‡ - æ¯ç§’è¯·æ±‚æ•°
    Errors: é”™è¯¯ç‡ - å¤±è´¥è¯·æ±‚çš„ç™¾åˆ†æ¯”
    Duration: æŒç»­æ—¶é—´ - è¯·æ±‚å¤„ç†æ—¶é—´
    """

    def __init__(self):
        self.rate = Rate()        # Requests per second
        self.errors = Errors()    # Error rate
        self.duration = Duration() # Response time

    def record_request(self, duration: float, success: bool):
        """è®°å½•ä¸€æ¬¡è¯·æ±‚"""
        self.rate.increment()
        if not success:
            self.errors.increment()
        self.duration.record(duration)

    def get_health(self) -> dict:
        """è¯„ä¼°ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        return {
            "requests_per_second": self.rate.get(),
            "error_rate": self.errors.get_rate(),
            "p50_latency": self.duration.percentile(50),
            "p95_latency": self.duration.percentile(95),
            "p99_latency": self.duration.percentile(99),
        }

# ä½¿ç”¨ç¤ºä¾‹
metrics = REDMetrics()

# è®°å½•è¯·æ±‚
start = time.time()
try:
    response = process_request()
    success = True
except Exception as e:
    success = False
finally:
    duration = time.time() - start
    metrics.record_request(duration, success)

# è·å–å¥åº·çŠ¶æ€
health = metrics.get_health()
print(f"QPS: {health['requests_per_second']}")
print(f"Error Rate: {health['error_rate']}")
print(f"P95 Latency: {health['p95_latency']}ms")
```

##### USE æ–¹æ³•

```python
# USE æ–¹æ³• - é€‚ç”¨äºèµ„æºé©±åŠ¨çš„ç³»ç»Ÿï¼ˆå¦‚æ•°æ®åº“ã€ç¼“å­˜ï¼‰
class USEMetrics:
    """
    Utilization: ä½¿ç”¨ç‡ - èµ„æºä½¿ç”¨ç™¾åˆ†æ¯”
    Saturation: é¥±å’Œåº¦ - èµ„æºæœ‰å¤šå¿™
    Errors: é”™è¯¯ - èµ„æºé”™è¯¯è®¡æ•°
    """

    def __init__(self):
        self.utilization = {}  # èµ„æºä½¿ç”¨ç‡
        self.saturation = {}   # èµ„æºé¥±å’Œåº¦
        self.errors = {}       # é”™è¯¯è®¡æ•°

    def record_cpu(self, used_percent: float, load_avg: float):
        """è®°å½• CPU æŒ‡æ ‡"""
        self.utilization["cpu"] = used_percent
        self.saturation["cpu"] = load_avg

    def record_memory(self, used_gb: float, total_gb: float):
        """è®°å½•å†…å­˜æŒ‡æ ‡"""
        self.utilization["memory"] = (used_gb / total_gb) * 100

    def record_disk(self, used_gb: float, total_gb: float, io_wait: float):
        """è®°å½•ç£ç›˜æŒ‡æ ‡"""
        self.utilization["disk"] = (used_gb / total_gb) * 100
        self.saturation["disk_io"] = io_wait

    def get_status(self) -> dict:
        """è·å–èµ„æºçŠ¶æ€"""
        return {
            "cpu_utilization": self.utilization.get("cpu", 0),
            "cpu_saturation": self.saturation.get("cpu", 0),
            "memory_utilization": self.utilization.get("memory", 0),
            "disk_utilization": self.utilization.get("disk", 0),
            "disk_io_saturation": self.saturation.get("disk_io", 0),
        }

# ä½¿ç”¨ç¤ºä¾‹
use_metrics = USEMetrics()

# ç›‘æ§èµ„æº
import psutil

use_metrics.record_cpu(
    used_percent=psutil.cpu_percent(),
    load_avg=psutil.getloadavg()[0]
)

use_metrics.record_memory(
    used_gb=psutil.virtual_memory().used / (1024**3),
    total_gb=psutil.virtual_memory().total / (1024**3)
)

status = use_metrics.get_status()
```

#### Prometheus å®æˆ˜

```python
# Prometheus å®¢æˆ·ç«¯ä½¿ç”¨
from prometheus_client import Counter, Histogram, Gauge, start_http_server

# å®šä¹‰æŒ‡æ ‡
http_requests_total = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

http_request_duration = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint']
)

active_connections = Gauge(
    'active_connections',
    'Number of active connections'
)

# ä¸­é—´ä»¶ä½¿ç”¨
from functools import wraps
import time

def track_requests(func):
    """è¿½è¸ªè¯·æ±‚çš„è£…é¥°å™¨"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start = time.time()

        # å¢åŠ æ´»è·ƒè¿æ¥
        active_connections.inc()

        try:
            result = func(*args, **kwargs)
            status = "success"
            return result
        except Exception as e:
            status = "error"
            raise
        finally:
            # è®°å½•è¯·æ±‚
            duration = time.time() - start
            http_request_duration.labels(
                method="POST",
                endpoint="/api/chat",
            ).observe(duration)

            http_requests_total.labels(
                method="POST",
                endpoint="/api/chat",
                status=status
            ).inc()

            # å‡å°‘æ´»è·ƒè¿æ¥
            active_connections.dec()

    return wrapper

# å¯åŠ¨ Prometheus æœåŠ¡å™¨
start_http_server(8000)

# ä½¿ç”¨ç¤ºä¾‹
@track_requests
def process_chat_request(message: str) -> str:
    """å¤„ç†èŠå¤©è¯·æ±‚"""
    # ä¸šåŠ¡é€»è¾‘
    return "AI response"
```

---

### 2. Logs (æ—¥å¿—)

#### å®šä¹‰

**Logs**: ç¦»æ•£çš„äº‹ä»¶è®°å½•ï¼ŒåŒ…å«æ—¶é—´æˆ³å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

#### ç»“æ„åŒ–æ—¥å¿—

```python
# ä¼ ç»Ÿæ—¥å¿— vs ç»“æ„åŒ–æ—¥å¿—

# âŒ ä¼ ç»Ÿæ—¥å¿—ï¼ˆéš¾ä»¥è§£æå’ŒæŸ¥è¯¢ï¼‰
print(f"User {user_id} logged in at {time.now()}")
print(f"ERROR: Database connection failed for user {user_id}")

# âœ… ç»“æ„åŒ–æ—¥å¿—ï¼ˆæ˜“äºè§£æå’ŒæŸ¥è¯¢ï¼‰
import structlog
import logging

# é…ç½® structlog
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    cache_logger_on_first_use=True,
)

# ä½¿ç”¨ç»“æ„åŒ–æ—¥å¿—
logger = structlog.get_logger()

# è®°å½•äº‹ä»¶
logger.info(
    "user_login",
    user_id=123,
    ip_address="192.168.1.1",
    user_agent="Mozilla/5.0...",
    timestamp="2024-01-15T10:30:00Z"
)

# è¾“å‡ºï¼ˆJSON æ ¼å¼ï¼‰:
{
  "event": "user_login",
  "user_id": 123,
  "ip_address": "192.168.1.1",
  "user_agent": "Mozilla/5.0...",
  "timestamp": "2024-01-15T10:30:00Z",
  "level": "info",
  "logger": "__main__"
}

# é”™è¯¯æ—¥å¿—
logger.error(
    "database_error",
    error_type="ConnectionError",
    user_id=123,
    query="SELECT * FROM users WHERE id = 123",
    retry_attempt=1,
    max_retries=3,
    stack_trace="..."  # è‡ªåŠ¨æ·»åŠ 
)
```

#### æ—¥å¿—çº§åˆ«å’Œæœ€ä½³å®è·µ

```python
# æ—¥å¿—çº§åˆ«ä½¿ç”¨æŒ‡å—
logger = structlog.get_logger()

# DEBUG: è¯¦ç»†çš„è¯Šæ–­ä¿¡æ¯
logger.debug(
    "ai_model_inference",
    model="gpt-4",
    input_tokens=150,
    max_tokens=1000,
    temperature=0.7,
    reasoning="é€‰æ‹©äº†è¾ƒä½çš„ temperature ä»¥ä¿è¯ç¨³å®šæ€§"
)

# INFO: ä¸€èˆ¬çš„ä¿¡æ¯äº‹ä»¶
logger.info(
    "request_completed",
    endpoint="/api/chat",
    method="POST",
    status_code=200,
    duration_ms=245,
    user_id=123
)

# WARNING: è­¦å‘Šäº‹ä»¶ï¼ˆä¸å½±å“åŠŸèƒ½ä½†éœ€è¦æ³¨æ„ï¼‰
logger.warning(
    "high_response_time",
    endpoint="/api/chat",
    duration_ms=1500,
    threshold_ms=1000,
    impact="ç”¨æˆ·ä½“éªŒå¯èƒ½å—å½±å“"
)

# ERROR: é”™è¯¯äº‹ä»¶ï¼ˆåŠŸèƒ½å¤±è´¥ä½†å¯ä»¥æ¢å¤ï¼‰
logger.error(
    "ai_api_error",
    error_type="RateLimitError",
    endpoint="/api/chat",
    retry_after=60,
    user_id=123,
    action="åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹"
)

# CRITICAL: ä¸¥é‡é”™è¯¯ï¼ˆç³»ç»Ÿæ— æ³•ç»§ç»­è¿è¡Œï¼‰
logger.critical(
    "database_connection_lost",
    error_type="ConnectionTimeout",
    retry_attempts=5,
    impact="æ‰€æœ‰æ•°æ®åº“æ“ä½œå¤±è´¥",
    action="éœ€è¦ç«‹å³äººå·¥ä»‹å…¥"
)
```

#### ä¸Šä¸‹æ–‡ä¿¡æ¯

```python
# æ·»åŠ è¯·æ±‚ä¸Šä¸‹æ–‡
from contextvars import ContextVar

request_context = ContextVar('request_context')

def log_with_context(**kwargs):
    """è®°å½•å¸¦æœ‰ä¸Šä¸‹æ–‡çš„æ—¥å¿—"""
    ctx = request_context.get({})
    logger.info(**kwargs, **ctx)

# ä¸­é—´ä»¶ï¼šè®¾ç½®è¯·æ±‚ä¸Šä¸‹æ–‡
@app.middleware("http")
async def add_request_context(request: Request, call_next):
    """ä¸ºæ¯ä¸ªè¯·æ±‚æ·»åŠ ä¸Šä¸‹æ–‡"""
    request_id = str(uuid.uuid4())

    # è®¾ç½®ä¸Šä¸‹æ–‡
    request_context.set({
        "request_id": request_id,
        "user_id": request.headers.get("X-User-ID"),
        "ip_address": request.client.host,
        "user_agent": request.headers.get("User-Agent"),
    })

    # è®°å½•è¯·æ±‚å¼€å§‹
    logger.info(
        "request_started",
        request_id=request_id,
        method=request.method,
        path=request.url.path
    )

    # å¤„ç†è¯·æ±‚
    try:
        response = await call_next(request)
        logger.info(
            "request_completed",
            request_id=request_id,
            status_code=response.status_code
        )
        return response
    except Exception as e:
        logger.error(
            "request_failed",
            request_id=request_id,
            error=str(e),
            error_type=type(e).__name__
        )
        raise

# åœ¨ä»»ä½•åœ°æ–¹ä½¿ç”¨ä¸Šä¸‹æ–‡æ—¥å¿—
def some_function():
    log_with_context(
        event="ai_inference",
        model="gpt-4",
        # request_context è‡ªåŠ¨æ·»åŠ 
    )
    # è¾“å‡ºä¼šåŒ…å«:
    # - event, model
    # - request_id, user_id, ip_address, user_agent
```

---

### 3. Traces (è¿½è¸ª)

#### å®šä¹‰

**Traces**: è¯·æ±‚åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­çš„å®Œæ•´è·¯å¾„ï¼Œå±•ç¤ºè¯·æ±‚å¦‚ä½•æµç»å¤šä¸ªæœåŠ¡ã€‚

#### æ ¸å¿ƒæ¦‚å¿µ

```python
# Trace çš„å±‚æ¬¡ç»“æ„
"""
Trace (è¿½è¸ª)
  â””â”€â”€ Span (è·¨åº¦) - å•ä¸ªæœåŠ¡çš„æ“ä½œ
        â”œâ”€â”€ Span ID - å”¯ä¸€æ ‡è¯†
        â”œâ”€â”€ Parent Span ID - çˆ¶ Span ID
        â”œâ”€â”€ Operation Name - æ“ä½œåç§°
        â”œâ”€â”€ Start Time - å¼€å§‹æ—¶é—´
        â”œâ”€â”€ Duration - æŒç»­æ—¶é—´
        â””â”€â”€ Tags - æ ‡ç­¾ï¼ˆå…ƒæ•°æ®ï¼‰
"""

# ç¤ºä¾‹ï¼šç”¨æˆ·è¯·æ±‚çš„å®Œæ•´è¿½è¸ª
"""
Trace: user_request_12345

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API Gateway (Span 1)                                        â”‚
â”‚ Duration: 1500ms                                            â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ Auth Service (Span 2)                â”‚                  â”‚
â”‚  â”‚ Duration: 200ms                      â”‚                  â”‚
â”‚  â”‚ Parent: Span 1                       â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ AI Chat Service (Span 3)             â”‚                  â”‚
â”‚  â”‚ Duration: 1200ms                     â”‚                  â”‚
â”‚  â”‚ Parent: Span 1                       â”‚                  â”‚
â”‚  â”‚                                      â”‚                  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚                  â”‚
â”‚  â”‚  â”‚ Database Query (Span 4)    â”‚     â”‚                  â”‚
â”‚  â”‚  â”‚ Duration: 150ms            â”‚     â”‚                  â”‚
â”‚  â”‚  â”‚ Parent: Span 3             â”‚     â”‚                  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚                  â”‚
â”‚  â”‚                                      â”‚                  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚                  â”‚
â”‚  â”‚  â”‚ OpenAI API Call (Span 5)   â”‚     â”‚                  â”‚
â”‚  â”‚  â”‚ Duration: 900ms            â”‚     â”‚                  â”‚
â”‚  â”‚  â”‚ Parent: Span 3             â”‚     â”‚                  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚                  â”‚
â”‚  â”‚                                      â”‚                  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚                  â”‚
â”‚  â”‚  â”‚ Cache Write (Span 6)       â”‚     â”‚                  â”‚
â”‚  â”‚  â”‚ Duration: 50ms             â”‚     â”‚                  â”‚
â”‚  â”‚  â”‚ Parent: Span 3             â”‚     â”‚                  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total Duration: 1500ms
Bottleneck: OpenAI API Call (900ms, 60% of total time)
"""
```

#### OpenTelemetry å®ç°

```python
# OpenTelemetry åˆ†å¸ƒå¼è¿½è¸ª
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter
from opentelemetry.sdk.resources import Resource

# é…ç½®èµ„æºï¼ˆæœåŠ¡ä¿¡æ¯ï¼‰
resource = Resource(attributes={
    "service.name": "ai-chat-service",
    "service.version": "1.0.0",
    "deployment.environment": "production"
})

# é…ç½® Tracer
trace.set_tracer_provider(TracerProvider(resource=resource))
tracer = trace.get_tracer(__name__)

# é…ç½®å¯¼å‡ºå™¨ï¼ˆå‘é€åˆ° Jaegerï¼‰
from opentelemetry.exporter.jaeger.thrift import JaegerExporter

jaeger_exporter = JaegerExporter(
    agent_host_name="localhost",
    agent_port=6831,
)

span_processor = BatchSpanProcessor(jaeger_exporter)
trace.get_tracer_provider().add_span_processor(span_processor)

# ä½¿ç”¨ Tracer
def process_chat_request(user_id: str, message: str) -> str:
    """å¤„ç†èŠå¤©è¯·æ±‚ï¼ˆå¸¦è¿½è¸ªï¼‰"""

    # åˆ›å»ºæ ¹ Span
    with tracer.start_as_current_span(
        "process_chat_request",
        attributes={
            "user.id": user_id,
            "message.length": len(message),
        }
    ) as parent_span:
        try:
            # Span 1: éªŒè¯ç”¨æˆ·
            with tracer.start_as_current_span("validate_user") as span:
                is_valid = validate_user(user_id)
                span.set_attribute("user.valid", is_valid)

                if not is_valid:
                    span.set_status(Status(StatusCode.ERROR))
                    raise ValueError("Invalid user")

            # Span 2: è·å–å†å²æ¶ˆæ¯
            with tracer.start_as_current_span("fetch_history") as span:
                history = fetch_chat_history(user_id)
                span.set_attribute("history.count", len(history))

            # Span 3: è°ƒç”¨ OpenAI API
            with tracer.start_as_current_span(
                "openai_api_call",
                attributes={
                    "model": "gpt-4",
                    "input.tokens": count_tokens(message + str(history))
                }
            ) as span:
                response = call_openai_api(message, history)

                # è®°å½•å“åº”è¯¦æƒ…
                span.set_attribute("output.tokens", response.usage.total_tokens)
                span.set_attribute("response.time", response.response_time)

            # Span 4: ä¿å­˜åˆ°æ•°æ®åº“
            with tracer.start_as_current_span("save_conversation") as span:
                save_conversation(user_id, message, response.text)

            return response.text

        except Exception as e:
            # è®°å½•å¼‚å¸¸
            parent_span.record_exception(e)
            parent_span.set_status(Status(StatusCode.ERROR, str(e)))
            raise

# è¿½è¸ªæ•°æ®åº“æŸ¥è¯¢
def fetch_chat_history(user_id: str) -> list:
    """è·å–èŠå¤©å†å²ï¼ˆå¸¦è¿½è¸ªï¼‰"""
    with tracer.start_as_current_span("db.query") as span:
        span.set_attribute("db.system", "postgresql")
        span.set_attribute("db.name", "chat_history")
        span.set_attribute("db.operation", "SELECT")

        start = time.time()
        try:
            result = db.execute(
                "SELECT * FROM messages WHERE user_id = ? ORDER BY created_at DESC LIMIT 10",
                user_id
            )
            duration = time.time() - start

            span.set_attribute("db.row_count", len(result))
            span.set_attribute("db.duration_ms", duration * 1000)

            return result
        except Exception as e:
            span.set_status(Status(StatusCode.ERROR))
            raise
```

---

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šAI åœ¨å¯è§‚æµ‹æ€§ä¸­çš„åº”ç”¨

### 1. å¼‚å¸¸æ£€æµ‹

#### Isolation Forest ç®—æ³•

```python
# ä½¿ç”¨æœºå™¨å­¦ä¹ æ£€æµ‹æŒ‡æ ‡å¼‚å¸¸
from sklearn.ensemble import IsolationForest
import numpy as np

class MetricsAnomalyDetector:
    """æŒ‡æ ‡å¼‚å¸¸æ£€æµ‹å™¨"""

    def __init__(self, contamination=0.1):
        """
        contamination: å¼‚å¸¸æ¯”ä¾‹ï¼ˆé¢„æœŸæœ‰å¤šå°‘æ•°æ®æ˜¯å¼‚å¸¸çš„ï¼‰
        """
        self.model = IsolationForest(
            contamination=contamination,
            random_state=42
        )
        self.is_trained = False

    def train(self, normal_metrics: list):
        """è®­ç»ƒæ¨¡å‹ï¼ˆä½¿ç”¨æ­£å¸¸æ•°æ®ï¼‰"""
        # normal_metrics æ ¼å¼: [
        #     [cpu_usage, memory_usage, response_time, error_rate],
        #     ...
        # ]
        X = np.array(normal_metrics)
        self.model.fit(X)
        self.is_trained = True

    def detect(self, current_metrics: list) -> dict:
        """æ£€æµ‹å½“å‰æŒ‡æ ‡æ˜¯å¦å¼‚å¸¸"""
        if not self.is_trained:
            return {"anomaly": False, "reason": "Model not trained"}

        X = np.array([current_metrics])
        prediction = self.model.predict(X)[0]  # 1=æ­£å¸¸, -1=å¼‚å¸¸
        score = self.model.score_samples(X)[0]  # å¼‚å¸¸åˆ†æ•°

        is_anomaly = prediction == -1

        return {
            "anomaly": is_anomaly,
            "score": float(score),
            "metrics": current_metrics,
            "timestamp": time.time()
        }

# ä½¿ç”¨ç¤ºä¾‹
detector = MetricsAnomalyDetector(contamination=0.05)

# æ”¶é›†æ­£å¸¸æ•°æ®è®­ç»ƒæ¨¡å‹
normal_data = [
    [45.2, 62.1, 120.5, 0.01],  # [cpu, memory, response_time, error_rate]
    [48.1, 65.3, 115.2, 0.00],
    [52.3, 68.9, 125.8, 0.02],
    # ... æ›´å¤šæ­£å¸¸æ•°æ®
]
detector.train(normal_data)

# å®æ—¶æ£€æµ‹
def monitor_system():
    """ç›‘æ§ç³»ç»Ÿå¹¶æ£€æµ‹å¼‚å¸¸"""
    while True:
        # è·å–å½“å‰æŒ‡æ ‡
        current = [
            get_cpu_usage(),
            get_memory_usage(),
            get_response_time(),
            get_error_rate()
        ]

        # æ£€æµ‹å¼‚å¸¸
        result = detector.detect(current)

        if result["anomaly"]:
            logger.critical(
                "anomaly_detected",
                score=result["score"],
                metrics=result["metrics"],
                alert="System behavior is abnormal!"
            )
            # è§¦å‘å‘Šè­¦

        time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æµ‹ä¸€æ¬¡

monitor_system()
```

#### æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹

```python
# ä½¿ç”¨ç»Ÿè®¡æ–¹æ³•æ£€æµ‹æ—¶é—´åºåˆ—å¼‚å¸¸
import pandas as pd
from scipy import stats

class TimeSeriesAnomalyDetector:
    """æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹å™¨"""

    def __init__(self, window_size=100, threshold=3):
        """
        window_size: æ»‘åŠ¨çª—å£å¤§å°
        threshold: æ ‡å‡†å·®é˜ˆå€¼å€æ•°
        """
        self.window_size = window_size
        self.threshold = threshold
        self.history = []

    def detect(self, value: float, timestamp: float) -> dict:
        """æ£€æµ‹æ–°å€¼æ˜¯å¦å¼‚å¸¸"""
        # æ·»åŠ åˆ°å†å²
        self.history.append((timestamp, value))

        # ä¿æŒçª—å£å¤§å°
        if len(self.history) > self.window_size:
            self.history.pop(0)

        # å¦‚æœæ•°æ®ä¸è¶³ï¼Œæ— æ³•æ£€æµ‹
        if len(self.history) < self.window_size:
            return {"anomaly": False, "reason": "Insufficient data"}

        # è®¡ç®—ç»Ÿè®¡é‡
        values = [v for _, v in self.history]
        mean = np.mean(values)
        std = np.std(values)

        # è®¡ç®—å½“å‰å€¼çš„ Z-score
        z_score = abs((value - mean) / std) if std > 0 else 0

        # åˆ¤æ–­æ˜¯å¦å¼‚å¸¸
        is_anomaly = z_score > self.threshold

        return {
            "anomaly": is_anomaly,
            "z_score": z_score,
            "value": value,
            "mean": mean,
            "std": std,
            "timestamp": timestamp,
            "severity": "HIGH" if z_score > self.threshold * 1.5 else "MEDIUM"
        }

# ä½¿ç”¨ç¤ºä¾‹
detector = TimeSeriesAnomalyDetector(window_size=100, threshold=3)

# ç›‘æ§å“åº”æ—¶é—´
def monitor_response_time():
    """ç›‘æ§ API å“åº”æ—¶é—´"""
    while True:
        # æµ‹é‡å“åº”æ—¶é—´
        start = time.time()
        try:
            response = make_request()
            response_time = (time.time() - start) * 1000  # æ¯«ç§’
        except Exception as e:
            response_time = 5000  # è¶…æ—¶

        # æ£€æµ‹å¼‚å¸¸
        result = detector.detect(response_time, time.time())

        if result["anomaly"]:
            logger.warning(
                "response_time_anomaly",
                current=result["value"],
                expected=f"{result['mean']:.2f}Â±{result['std']:.2f}",
                z_score=result["z_score"],
                severity=result["severity"]
            )

            # å¦‚æœä¸¥é‡å¼‚å¸¸ï¼Œè§¦å‘å‘Šè­¦
            if result["severity"] == "HIGH":
                send_alert(f"High response time: {result['value']:.2f}ms")

        time.sleep(10)  # æ¯ 10 ç§’æ£€æµ‹ä¸€æ¬¡
```

---

### 2. æ—¥å¿—åˆ†æ

#### ä½¿ç”¨ LLM åˆ†æé”™è¯¯æ—¥å¿—

```python
# ä½¿ç”¨ LLM è¿›è¡Œæ™ºèƒ½æ—¥å¿—åˆ†æ
import openai

class LogAnalyzer:
    """æ™ºèƒ½æ—¥å¿—åˆ†æå™¨"""

    def __init__(self, api_key: str):
        self.client = openai.OpenAI(api_key=api_key)

    def analyze_error(self, error_log: str, context: dict = None) -> dict:
        """åˆ†æé”™è¯¯æ—¥å¿—"""

        prompt = f"""
åˆ†æä»¥ä¸‹é”™è¯¯æ—¥å¿—ï¼Œæä¾›ï¼š
1. é”™è¯¯ç±»å‹
2. å¯èƒ½çš„æ ¹æœ¬åŸå› 
3. å»ºè®®çš„ä¿®å¤æ–¹æ¡ˆ
4. ä¸¥é‡ç¨‹åº¦è¯„ä¼°ï¼ˆCRITICAL/HIGH/MEDIUM/LOWï¼‰

é”™è¯¯æ—¥å¿—ï¼š
```
{error_log}
```

{f'ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š{context}' if context else ''}

è¯·ä»¥ JSON æ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "error_type": "...",
    "root_cause": "...",
    "suggested_fix": "...",
    "severity": "...",
    "confidence": 0.0-1.0
}}
"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è½¯ä»¶å·¥ç¨‹å¸ˆå’Œç³»ç»Ÿç®¡ç†å‘˜ï¼Œæ“…é•¿åˆ†æé”™è¯¯æ—¥å¿—ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,  # é™ä½éšæœºæ€§
            )

            result = json.loads(response.choices[0].message.content)

            logger.info(
                "log_analysis_completed",
                error_type=result["error_type"],
                severity=result["severity"],
                confidence=result["confidence"]
            )

            return result

        except Exception as e:
            logger.error("log_analysis_failed", error=str(e))
            return {
                "error_type": "Unknown",
                "root_cause": "Analysis failed",
                "suggested_fix": "Manual review required",
                "severity": "MEDIUM",
                "confidence": 0.0
            }

# ä½¿ç”¨ç¤ºä¾‹
analyzer = LogAnalyzer(api_key=os.getenv("OPENAI_API_KEY"))

# åˆ†æé”™è¯¯
error_log = """
Traceback (most recent call last):
  File "/app/main.py", line 45, in process_request
    response = openai.ChatCompletion.create(...)
  File "/app/venv/lib/python3.9/site-packages/openai/api_resources/chat/completion.py", line 25, in create
    response, _, api_key = requestor.request(...)
  File "/app/venv/lib/python3.9/site-packages/openai/api_requestor.py", line 298, in request
    raise error_type(err, resp)
openai.error.RateLimitError: Rate limit reached for requests

The above exception was the direct cause of the following exception:

...
"""

analysis = analyzer.analyze_error(
    error_log,
    context={
        "service": "ai-chat-service",
        "endpoint": "/api/chat",
        "recent_traffic": "500 requests/min",
    }
)

print(f"Error Type: {analysis['error_type']}")
print(f"Root Cause: {analysis['root_cause']}")
print(f"Suggested Fix: {analysis['suggested_fix']}")
print(f"Severity: {analysis['severity']}")
```

#### æ—¥å¿—æ¨¡å¼è¯†åˆ«

```python
# è‡ªåŠ¨è¯†åˆ«æ—¥å¿—æ¨¡å¼
import re
from collections import defaultdict, Counter

class LogPatternAnalyzer:
    """æ—¥å¿—æ¨¡å¼åˆ†æå™¨"""

    def __init__(self):
        self.patterns = defaultdict(int)
        self.templates = {}

    def extract_template(self, log_message: str) -> str:
        """æå–æ—¥å¿—æ¨¡æ¿ï¼ˆå°†å˜é‡æ›¿æ¢ä¸ºå ä½ç¬¦ï¼‰"""
        # æ›¿æ¢æ•°å­—
        template = re.sub(r'\d+', '<NUM>', log_message)
        # æ›¿æ¢ UUID
        template = re.sub(
            r'[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}',
            '<UUID>',
            template
        )
        # æ›¿æ¢é‚®ç®±
        template = re.sub(r'\S+@\S+', '<EMAIL>', template)
        # æ›¿æ¢ URL
        template = re.sub(r'https?://\S+', '<URL>', template)
        # æ›¿æ¢æ–‡ä»¶è·¯å¾„
        template = re.sub(r'/[\w/.-]+', '<PATH>', template)
        # æ›¿æ¢ IP åœ°å€
        template = re.sub(r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}', '<IP>', template)

        return template

    def add_log(self, log_message: str):
        """æ·»åŠ æ—¥å¿—å¹¶æ›´æ–°æ¨¡å¼ç»Ÿè®¡"""
        template = self.extract_template(log_message)
        self.patterns[template] += 1

        # ä¿å­˜ç¤ºä¾‹
        if template not in self.templates:
            self.templates[template] = log_message

    def get_top_patterns(self, n=10) -> list:
        """è·å–æœ€å¸¸è§çš„ N ä¸ªæ¨¡å¼"""
        sorted_patterns = sorted(
            self.patterns.items(),
            key=lambda x: x[1],
            reverse=True
        )

        return [
            {
                "template": template,
                "count": count,
                "example": self.templates[template]
            }
            for template, count in sorted_patterns[:n]
        ]

# ä½¿ç”¨ç¤ºä¾‹
analyzer = LogPatternAnalyzer()

# åˆ†ææ—¥å¿—æµ
log_lines = [
    "User 12345 logged in from 192.168.1.1",
    "User 67890 logged in from 192.168.1.2",
    "Request 550e8400-e29b-41d4-a716-446655440000 failed with status 500",
    "Request 6ba7b810-9dad-11d1-80b4-00c04fd430c8 failed with status 500",
    "Database connection to db.example.com:5432 timed out",
    "Database connection to db.example.com:5432 timed out",
    "API call to https://api.openai.com/v1/chat/completions failed",
]

for log in log_lines:
    analyzer.add_log(log)

# è·å–å¸¸è§æ¨¡å¼
patterns = analyzer.get_top_patterns(5)

for i, pattern in enumerate(patterns, 1):
    print(f"\nPattern #{i} (Count: {pattern['count']})")
    print(f"Template: {pattern['template']}")
    print(f"Example: {pattern['example']}")

# è¾“å‡º:
# Pattern #1 (Count: 3)
# Template: User <NUM> logged in from <IP>
# Example: User 12345 logged in from 192.168.1.1
#
# Pattern #2 (Count: 2)
# Template: Database connection to <PATH>:<NUM> timed out
# Example: Database connection to db.example.com:5432 timed out
```

---

### 3. Trace åˆ†æ

#### æ€§èƒ½ç“¶é¢ˆè¯†åˆ«

```python
# åˆ†æè¿½è¸ªæ•°æ®ï¼Œè¯†åˆ«æ€§èƒ½ç“¶é¢ˆ
class TraceAnalyzer:
    """è¿½è¸ªåˆ†æå™¨"""

    def __init__(self):
        self.traces = []

    def add_trace(self, trace_data: dict):
        """æ·»åŠ è¿½è¸ªæ•°æ®"""
        self.traces.append(trace_data)

    def analyze_bottlenecks(self) -> list:
        """åˆ†ææ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []

        for trace in self.traces:
            # åˆ†ææ¯ä¸ª Span
            for span in trace["spans"]:
                duration = span["duration_ms"]

                # å¦‚æœæŒç»­æ—¶é—´è¶…è¿‡é˜ˆå€¼ï¼Œè§†ä¸ºç“¶é¢ˆ
                if duration > 500:  # 500ms
                    bottlenecks.append({
                        "trace_id": trace["trace_id"],
                        "span_name": span["name"],
                        "duration_ms": duration,
                        "percentage_of_total": (duration / trace["total_duration"]) * 100,
                        "service": span.get("service", "unknown"),
                        "attributes": span.get("attributes", {})
                    })

        # æŒ‰æŒç»­æ—¶é—´æ’åº
        bottlenecks.sort(key=lambda x: x["duration_ms"], reverse=True)
        return bottlenecks

    def get_slowest_operations(self, n=10) -> list:
        """è·å–æœ€æ…¢çš„ N ä¸ªæ“ä½œ"""
        all_spans = []

        for trace in self.traces:
            for span in trace["spans"]:
                all_spans.append({
                    "operation": span["name"],
                    "duration_ms": span["duration_ms"],
                    "service": span.get("service", "unknown"),
                    "trace_id": trace["trace_id"]
                })

        # æ’åºå¹¶è¿”å›å‰ N ä¸ª
        all_spans.sort(key=lambda x: x["duration_ms"], reverse=True)
        return all_spans[:n]

# ä½¿ç”¨ç¤ºä¾‹
analyzer = TraceAnalyzer()

# æ·»åŠ è¿½è¸ªæ•°æ®
analyzer.add_trace({
    "trace_id": "trace-123",
    "total_duration": 1500,
    "spans": [
        {
            "name": "api_gateway",
            "duration_ms": 1500,
            "service": "api-gateway"
        },
        {
            "name": "auth_service",
            "duration_ms": 200,
            "service": "auth-service"
        },
        {
            "name": "openai_api_call",
            "duration_ms": 900,
            "service": "ai-service",
            "attributes": {
                "model": "gpt-4",
                "input_tokens": 150,
                "output_tokens": 300
            }
        },
        {
            "name": "db_query",
            "duration_ms": 350,
            "service": "ai-service"
        }
    ]
})

# åˆ†æç“¶é¢ˆ
bottlenecks = analyzer.analyze_bottlenecks()

print("Performance Bottlenecks:")
for i, bottleneck in enumerate(bottlenecks, 1):
    print(f"\n{i}. {bottleneck['span_name']}")
    print(f"   Duration: {bottleneck['duration_ms']}ms")
    print(f"   Percentage: {bottleneck['percentage_of_total']:.1f}%")
    print(f"   Service: {bottleneck['service']}")

# è¾“å‡º:
# Performance Bottlenecks:
#
# 1. openai_api_call
#    Duration: 900ms
#    Percentage: 60.0%
#    Service: ai-service
#
# 2. db_query
#    Duration: 350ms
#    Percentage: 23.3%
#    Service: ai-service
```

---

## ğŸ“Š çŸ¥è¯†æ£€æŸ¥

### è‡ªæˆ‘è¯„ä¼°é—®é¢˜

1. **å¯è§‚æµ‹æ€§çš„ä¸‰å¤§æ”¯æŸ±æ˜¯ä»€ä¹ˆï¼Ÿå®ƒä»¬å„è‡ªçš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ**

2. **RED æ–¹æ³•å’Œ USE æ–¹æ³•åˆ†åˆ«é€‚ç”¨äºä»€ä¹ˆåœºæ™¯ï¼Ÿ**

3. **å¦‚ä½•ä½¿ç”¨ç»“æ„åŒ–æ—¥å¿—æå‡æ—¥å¿—æŸ¥è¯¢å’Œåˆ†ææ•ˆç‡ï¼Ÿ**

4. **åˆ†å¸ƒå¼è¿½è¸ªå¦‚ä½•å¸®åŠ©è¯†åˆ«æ€§èƒ½ç“¶é¢ˆï¼Ÿ**

5. **å¦‚ä½•ä½¿ç”¨æœºå™¨å­¦ä¹ æ£€æµ‹æŒ‡æ ‡å¼‚å¸¸ï¼Ÿ**

6. **LLM å¦‚ä½•å¸®åŠ©åˆ†æé”™è¯¯æ—¥å¿—ï¼Ÿ**

---

## ğŸ“š å»¶ä¼¸é˜…è¯»

### èµ„æº

1. [Google SRE Book - Monitoring](https://sre.google/sre-book/monitoring-distributed-systems/)
2. [OpenTelemetry Documentation](https://opentelemetry.io/docs)
3. [Prometheus Best Practices](https://prometheus.io/docs/practices/)

### å·¥å…·

1. **Prometheus**: å¼€æºç›‘æ§ç³»ç»Ÿ
2. **Grafana**: å¯è§†åŒ–ä»ªè¡¨æ¿
3. **Jaeger**: åˆ†å¸ƒå¼è¿½è¸ªå¹³å°
4. **Loki**: æ—¥å¿—èšåˆç³»ç»Ÿ

---

**ä¸‹ä¸€é˜…è¯»**: [è‡ªåŠ¨åŒ–äº‹ä»¶å“åº”ä¸è‡ªæ„ˆåˆç³»ç»Ÿ](./02-automated-incident-response-and-self-healing.md)
